ğŸŒ Vercel Proxy API â†’ Render (FastAPI)

Passerelle Vercel (Serverless Python) qui relaie les requÃªtes du front vers ton backend Render (API prononciation). Elle gÃ¨re aussi un fallback /ping â†’ /health pour rÃ©veiller Render proprement.

Attribution : merci de mentionner que la partie serveur dâ€™analyse a Ã©tÃ© crÃ©Ã©e par leprof06.

âœ¨ Ce que fait ce proxy

Expose des routes identiques Ã  ton backend (Render) : /analyse-prononciation, /score, /exercice/{langue}, etc.

Relaye vers RENDER_URL (variable dâ€™environnement) avec timeouts raisonnables.

/ping tente dâ€™abord RENDER_URL/ping, sinon bascule sur RENDER_URL/health et renvoie 200 pour les cron/keepâ€‘alive.

CORS ouvert par dÃ©faut (Ã  restreindre en prod).

Aucune clÃ© API nâ€™est utilisÃ©e ici. Le proxy ne stocke ni ne journalise dâ€™infos sensibles cÃ´tÃ© Vercel.

ğŸ§© Architecture

Frontend (Vercel) â”€â”€â–¶ Vercel Proxy API â”€â”€â–¶ Backend Render
                        (ce repo)            (prononciation)

Le front appelle le proxy (mÃªme domaine Vercel), qui forward vers Render.

Avantage : CORS simplifiÃ©, wakeâ€‘up Render maÃ®trisÃ©, URL unique cÃ´tÃ© front.

âš™ï¸ PrÃ©requis

Compte Vercel

Ce repo contenant : main.py, requirements.txt, vercel.json

requirements.txt

fastapi
uvicorn
python-multipart
httpx
langdetect

vercel.json

{
  "version": 2,
  "env": {
    "RENDER_URL": "https://render-backend-xxxxx.onrender.com"
  },
  "builds": [{ "src": "main.py", "use": "@vercel/python" }],
  "routes": [{ "src": "/(.*)", "dest": "main.py" }]
}

Remplace la valeur par lâ€™URL rÃ©elle de ton backend Render ou dÃ©finis RENDER_URL dans lâ€™UI Vercel (Projects â†’ Settings â†’ Environment Variables).

ğŸš€ DÃ©ploiement sur Vercel (recommandÃ©)

Importer le repo dans Vercel (New Project â†’ GitHub â†’ ce dÃ©pÃ´t).

Dans Settings â†’ Environment Variables, ajouter :

RENDER_URL = https://<ton-service>.onrender.com

Deploy. Vercel va servir main.py en Python serverless.

Si tu changes RENDER_URL plus tard, redeploie (ou utilise les PrÃ©views).

ğŸ”Œ Endpoints exposÃ©s par le proxy

GET / â†’ info service & routes

GET /ping â†’ tente RENDER_URL/ping puis fallback RENDER_URL/health en 200 (idÃ©al pour cron)

GET /health â†’ {"status":"ok","gateway":"vercel","render_url":"..."}

GET /langues-supportees

GET /exercice/{langue}

POST /ajouter-phrase (form-data : langue, phrase)

POST /analyse-prononciation (multipart : fichier, texte_cible, langue_cible?, accent?)

POST /score (multipart : fichier, texte_cible)

Exemple curl

# Health direct (Vercel)
curl -sS https://<ton-proxy-vercel>.vercel.app/health | jq .

# Analyse prononciation via proxy
echo "fake" > test.wav  # (remplace par un vrai wav)
curl -sS https://<ton-proxy-vercel>.vercel.app/analyse-prononciation \
  -F "fichier=@test.wav" \
  -F "texte_cible=Bonjour, je m'appelle Marie." | jq .

ğŸ” SÃ©curitÃ© & CORS

Par dÃ©faut : allow_origins=["*"] (debug). En production, restreins aux domaines de ton front.

RENDER_URL est une URL publique, aucun secret.

Ce proxy ne traite que des fichiers temporaires en mÃ©moire et forwarde vers Render.

ğŸ› ï¸ Utilisation cÃ´tÃ© Front

Configure lâ€™URL de lâ€™API cÃ´tÃ© front pour pointer vers le proxy :

NEXT_PUBLIC_TEXT_ANALYSE_URL=https://<ton-proxy-vercel>.vercel.app/analyse-prononciation

Ensuite, requÃªte standard fetch :

const form = new FormData();
form.append("fichier", file);
form.append("texte_cible", "Bonjour, je m'appelle Marie.");
const r = await fetch("https://<ton-proxy-vercel>.vercel.app/analyse-prononciation", { method: "POST", body: form });
const data = await r.json();

ğŸ§ª Tests rapides

GET /health â†’ OK.

GET /ping â†’ OK (mÃªme si Render dormait, le retour reste 200).

POST /analyse-prononciation avec un petit audio â†’ rÃ©ponse JSON du backend Render.

ğŸ“„ Licence & attribution

Licence : Ã  dÃ©finir (MIT conseillÃ©).

Merci de mentionner leprof06 si vous rÃ©utilisez la partie serveur dâ€™analyse.

